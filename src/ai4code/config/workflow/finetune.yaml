
name: finetune
args:
  data:
    max_data_size: 10
    test_ratio: 0.1
  tokenizer:
    max_len: 128
  training:
    learning_rate: 2e-5
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 128
    num_train_epochs: 1
    weight_decay: 0.01
    warmup_ratio: 0.1
    fp16: true
    gradient_checkpointing: false
    early_stopping_patience: 1
  inference:
    batch_size: 128
  model_name: distilbert-base-uncased